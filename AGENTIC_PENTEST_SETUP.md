# Agentic Pentest Demo - Setup Guide

## Overview
This workflow uses **Ollama** (local LLM) to create an agentic pentest automation:
1. User submits any target (domain, IP, URL, company name)
2. AI analyzes and plans which recon tools to run
3. Commands execute automatically
4. AI generates a professional security report

## Quick Start

### 1. Restart Docker Stack with Ollama

```bash
cd /Users/tester/Documents/n8n_stuff
docker-compose down
docker-compose up -d
```

### 2. Pull Ollama Model

Wait for Ollama container to start, then:

```bash
# Pull llama3.2 (3B parameters, good balance of speed/quality)
docker exec recon_ollama ollama pull llama3.2

# Or for better results (but slower):
# docker exec recon_ollama ollama pull llama3.1:8b
```

### 3. Import Workflow into n8n

1. Open n8n: http://localhost:5678
2. Go to **Workflows** → **Import from File**
3. Select: `workflows/agentic_pentest_demo.json`
4. **Activate** the workflow (toggle switch)

### 4. Test the Workflow

```bash
# Test with a domain
curl -X POST http://localhost/webhook/pentest \
  -H "Content-Type: application/json" \
  -d '{"target": "example.com"}'

# Test with an IP
curl -X POST http://localhost/webhook/pentest \
  -H "Content-Type: application/json" \
  -d '{"target": "8.8.8.8"}'

# Test with a URL
curl -X POST http://localhost/webhook/pentest \
  -H "Content-Type: application/json" \
  -d '{"target": "https://testphp.vulnweb.com"}'
```

## Workflow Architecture

```
[Webhook: /pentest]
       ↓
[Ollama: Analyze target, plan commands]
       ↓
[Code: Parse JSON plan into command items]
       ↓
[Split In Batches: Loop through commands]
       ↓
[Execute Command] ←──── [Loop back]
       ↓
[Code: Aggregate all results]
       ↓
[Ollama: Generate security report]
       ↓
[Respond: Return JSON with report]
```

## Response Format

```json
{
  "success": true,
  "report": "# Security Assessment Report\n\n## Executive Summary...",
  "target": "example.com",
  "target_type": "domain",
  "commands_executed": 4,
  "timestamp": "2025-01-20T10:30:00.000Z"
}
```

## Available Tools

The AI can choose from these tools based on target type:

| Tool | Purpose |
|------|---------|
| `subfinder` | Fast subdomain enumeration |
| `amass` | Passive DNS reconnaissance |
| `nmap` | Port scanning and service detection |
| `nuclei` | Vulnerability scanning |
| `httpx` | HTTP probing and tech detection |
| `whois` | Domain registration info |
| `dig` | DNS record queries |

## Customization

### Change Ollama Model

Edit the workflow's HTTP Request nodes and change `llama3.2` to:
- `llama3.1:8b` - Better quality, slower
- `mistral` - Good alternative
- `codellama` - If you want code-focused analysis

### Add More Tools

Edit the "AI Recon Planner" node's prompt to include additional tools like:
- `katana` - JavaScript endpoint discovery
- `ffuf` - Directory fuzzing
- `testssl.sh` - SSL/TLS analysis

### Adjust Timeouts

Default timeouts:
- Ollama planning: 120 seconds
- Command execution: 300 seconds
- Ollama report: 180 seconds

Edit the `timeout` option in respective nodes if needed.

## Troubleshooting

### Ollama not responding
```bash
# Check Ollama container
docker logs recon_ollama

# Test Ollama API
curl http://localhost:11434/api/tags
```

### Model not found
```bash
# List available models
docker exec recon_ollama ollama list

# Pull if missing
docker exec recon_ollama ollama pull llama3.2
```

### Commands failing
Check n8n execution logs for specific command errors. Common issues:
- Target not reachable
- Tool timeout (increase timeout in node)
- Permission issues (tools run as node user)

## Performance Notes

- First request is slower (model loading)
- Subsequent requests use cached model
- Expect 30-120 seconds total depending on:
  - Number of commands AI plans
  - Target responsiveness
  - Ollama model speed

## Security Considerations

- Only use against authorized targets
- Webhook is publicly accessible on port 80
- Consider adding authentication for production use
- Results may contain sensitive reconnaissance data
