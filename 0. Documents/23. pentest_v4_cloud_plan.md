# Pentest V4 Cloud Scanner - Implementation Plan

## Overview

Create `workflows/pentest_v4_cloud.json` by copying and modifying `pentest_router_v3.json` to support Azure cloud security testing.

---

## 1. Components to Keep from V3

| Component | Purpose |
|-----------|---------|
| Webhook trigger | Entry point with project_id |
| Project validation | Verify project exists and is active |
| Rate limiting | Prevent abuse (reuse existing logic) |
| Scan job creation | Track scan status in database |
| Audit logging | Record all actions |
| AI scan planner | Generate intelligent scan commands |
| Result aggregation | Collect outputs from all scans |
| Report generation | Summarize findings |

---

## 2. Components to Remove

| Component | Reason |
|-----------|--------|
| Tunnel selection | Azure is API-based, not network-based |
| ProxyChains routing | Not needed for cloud APIs |
| SSH pivot logic | No network pivoting in cloud |
| Nmap/port scanning | Different attack surface |

---

## 3. New Scan Modes

| Mode | Description | Tools |
|------|-------------|-------|
| `aad-enum` | Azure AD enumeration | ROADtools, AADInternals |
| `resource-discovery` | Find all Azure resources | az cli, ScoutSuite |
| `blob-scan` | Storage account enumeration | MicroBurst, BlobHunter |
| `privilege-audit` | Check IAM misconfigurations | AzureHound, Stormspotter |
| `keyvault-access` | Test Key Vault permissions | az keyvault |
| `api-abuse` | Graph API testing | GraphRunner |
| `full-cloud` | All of the above | All tools |

---

## 4. Input Parameters

```json
{
  "project_id": "uuid",
  "tenant_id": "azure-tenant-guid",
  "subscription_id": "azure-subscription-guid (optional)",
  "credential_id": "uuid (reference to secure_credentials)",
  "scan_mode": "aad-enum|resource-discovery|blob-scan|privilege-audit|keyvault-access|api-abuse|full-cloud",
  "target_scope": ["specific resource groups or all"],
  "options": {
    "include_deleted": false,
    "check_public_access": true,
    "enumerate_guests": true
  }
}
```

---

## 5. Credential Types for Azure

Add to `secure_credentials` table:

| Type | Fields |
|------|--------|
| `azure_service_principal` | client_id, client_secret, tenant_id |
| `azure_access_token` | access_token, refresh_token, expires_at |
| `azure_managed_identity` | resource_id (for VM-based testing) |
| `azure_certificate` | cert_path, cert_password, tenant_id |

---

## 6. New Database Migration (007_phase_e_cloud.sql)

### Tables

```sql
-- Azure tenants being tested
CREATE TABLE azure_tenants (
  id UUID PRIMARY KEY,
  project_id UUID REFERENCES projects(id),
  tenant_id VARCHAR(36) NOT NULL,
  tenant_name VARCHAR(255),
  verified_domains JSONB,
  discovered_at TIMESTAMP DEFAULT NOW()
);

-- Azure subscriptions
CREATE TABLE azure_subscriptions (
  id UUID PRIMARY KEY,
  tenant_id UUID REFERENCES azure_tenants(id),
  subscription_id VARCHAR(36) NOT NULL,
  subscription_name VARCHAR(255),
  state VARCHAR(50),
  discovered_at TIMESTAMP DEFAULT NOW()
);

-- Discovered Azure resources
CREATE TABLE azure_resources (
  id UUID PRIMARY KEY,
  subscription_id UUID REFERENCES azure_subscriptions(id),
  resource_id VARCHAR(500) NOT NULL,
  resource_type VARCHAR(255),
  resource_name VARCHAR(255),
  resource_group VARCHAR(255),
  location VARCHAR(100),
  properties JSONB,
  tags JSONB,
  public_access BOOLEAN DEFAULT false,
  discovered_at TIMESTAMP DEFAULT NOW()
);

-- Azure AD objects (users, groups, service principals)
CREATE TABLE azure_ad_objects (
  id UUID PRIMARY KEY,
  tenant_id UUID REFERENCES azure_tenants(id),
  object_id VARCHAR(36) NOT NULL,
  object_type VARCHAR(50),
  display_name VARCHAR(255),
  upn VARCHAR(255),
  properties JSONB,
  discovered_at TIMESTAMP DEFAULT NOW()
);

-- Azure role assignments
CREATE TABLE azure_role_assignments (
  id UUID PRIMARY KEY,
  subscription_id UUID REFERENCES azure_subscriptions(id),
  principal_id VARCHAR(36),
  principal_type VARCHAR(50),
  role_definition_id VARCHAR(255),
  role_name VARCHAR(255),
  scope VARCHAR(500),
  discovered_at TIMESTAMP DEFAULT NOW()
);

-- Cloud-specific findings
CREATE TABLE cloud_findings (
  id UUID PRIMARY KEY,
  project_id UUID REFERENCES projects(id),
  resource_id UUID REFERENCES azure_resources(id),
  finding_type VARCHAR(100),
  severity severity_level,
  title VARCHAR(500),
  description TEXT,
  remediation TEXT,
  evidence JSONB,
  discovered_at TIMESTAMP DEFAULT NOW()
);
```

### Views

```sql
-- Public resources summary
CREATE VIEW public_azure_resources AS
SELECT * FROM azure_resources WHERE public_access = true;

-- Privileged role assignments
CREATE VIEW privileged_assignments AS
SELECT * FROM azure_role_assignments
WHERE role_name IN ('Owner', 'Contributor', 'User Access Administrator');
```

---

## 7. Dockerfile Additions

```dockerfile
# Azure tools
RUN apk add --no-cache \
    powershell \
    azure-cli

# Python Azure tools
RUN pip3 install --no-cache-dir --break-system-packages \
    roadtools \
    scoutSuite \
    microsoftgraph-python

# PowerShell modules (for AADInternals)
RUN pwsh -Command "Install-Module -Name AADInternals -Force -Scope AllUsers"
RUN pwsh -Command "Install-Module -Name Az -Force -Scope AllUsers"
```

---

## 8. Workflow Structure

### Nodes to Create

1. **Webhook Trigger** - Accept cloud scan requests
2. **Validate Project** - Check project exists (keep from v3)
3. **Load Azure Credential** - Fetch from secure_credentials + Redis
4. **Authenticate to Azure** - az login with service principal
5. **Check Rate Limits** - Reuse from v3
6. **Create Scan Job** - Record in database
7. **AI Cloud Planner** - Generate Azure-specific commands
8. **Build Cloud Commands** - Map modes to tool commands
9. **Execute Cloud Scans** - Run az cli, ScoutSuite, etc.
10. **Parse Cloud Results** - Extract resources, findings
11. **Save to Database** - Insert into azure_* tables
12. **Generate Cloud Report** - Summarize findings
13. **Cleanup** - Logout, clear tokens

### Command Mapping (Build Cloud Commands node)

```javascript
switch (scanMode) {
  case 'aad-enum':
    commands = [
      `az ad user list --output json`,
      `az ad group list --output json`,
      `az ad sp list --all --output json`,
      `roadrecon auth -t ${tenantId}`,
      `roadrecon gather`
    ];
    break;

  case 'resource-discovery':
    commands = [
      `az resource list --output json`,
      `az storage account list --output json`,
      `az keyvault list --output json`,
      `az vm list --output json`,
      `scout suite azure --no-browser`
    ];
    break;

  case 'blob-scan':
    commands = [
      `az storage account list --output json`,
      `az storage container list --account-name {account} --output json`,
      `az storage blob list --container-name {container} --account-name {account}`
    ];
    break;

  case 'privilege-audit':
    commands = [
      `az role assignment list --all --output json`,
      `az ad app list --all --output json`,
      `azurehound -t ${tenantId}`
    ];
    break;

  case 'keyvault-access':
    commands = [
      `az keyvault list --output json`,
      `az keyvault secret list --vault-name {vault} --output json`,
      `az keyvault key list --vault-name {vault} --output json`
    ];
    break;
}
```

---

## 9. Result Parsing Logic

### AAD Enumeration Results

```javascript
// Parse az ad user list
const users = JSON.parse(stdout);
for (const user of users) {
  await insertAzureADObject({
    tenant_id: tenantId,
    object_id: user.id,
    object_type: 'user',
    display_name: user.displayName,
    upn: user.userPrincipalName,
    properties: user
  });
}
```

### Finding Detection

```javascript
// Check for public blob containers
if (container.publicAccess !== 'None') {
  await createCloudFinding({
    finding_type: 'public_storage',
    severity: 'high',
    title: `Public blob container: ${container.name}`,
    description: 'Container allows anonymous access',
    remediation: 'Set publicAccess to None'
  });
}

// Check for overprivileged service principals
if (roleAssignment.roleName === 'Owner' && principalType === 'ServicePrincipal') {
  await createCloudFinding({
    finding_type: 'excessive_privilege',
    severity: 'high',
    title: `Service Principal with Owner role`,
    remediation: 'Apply least privilege principle'
  });
}
```

---

## 10. Implementation Steps

### Phase 1: Database
1. Create `migrations/007_phase_e_cloud.sql`
2. Add Azure credential types to existing tables

### Phase 2: Infrastructure
1. Update Dockerfile with Azure CLI, PowerShell, Python tools
2. Update docker-compose if needed
3. Test tool installations

### Phase 3: Workflow
1. Copy `pentest_router_v3.json` to `pentest_v4_cloud.json`
2. Remove tunnel/proxy nodes
3. Update webhook parameters
4. Create Azure authentication node
5. Replace command builder with cloud commands
6. Update AI planner prompt for cloud context
7. Create cloud result parsers
8. Update database save nodes for azure_* tables
9. Update report generation for cloud findings

### Phase 4: Integration
1. Update `credential_tester.json` for Azure credential validation
2. Create `azure_housekeeping.json` for token refresh
3. Update web interface for cloud scanning

### Phase 5: Documentation
1. Update README.md
2. Update setup.sh
3. Add Azure-specific setup instructions

---

## 11. Security Considerations

- **Token storage**: Azure tokens in Redis with short TTL (1 hour)
- **Credential rotation**: Auto-refresh before expiry
- **Scope limitation**: Only test subscriptions in project scope
- **Audit trail**: Log all Azure API calls
- **Rate limiting**: Respect Azure API throttling limits

---

## 12. Future Enhancements

- AWS support (aws cli, Prowler, ScoutSuite)
- GCP support (gcloud, ScoutSuite)
- Multi-cloud correlation
- Automated remediation suggestions
- Integration with Azure Sentinel

---

## Questions Before Implementation

1. **Which Azure tools to prioritize?** (ScoutSuite vs custom az cli vs both)
2. **Token handling**: Store in Redis or re-authenticate per scan?
3. **Multi-subscription**: Scan all subscriptions or require specific selection?
4. **Finding severity mapping**: Use Azure-specific or align with existing severity_level?
5. **Report format**: Separate cloud report or integrate with existing?
